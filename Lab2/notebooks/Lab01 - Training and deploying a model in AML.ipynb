{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e43d337",
   "metadata": {},
   "source": [
    "# Lab01 - Training and deploying a model in AML\n",
    "\n",
    "In this lab, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. We will use data transformations and the GradientBoostingRegressor algorithm from the scikit-learn library to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n",
    "\n",
    "The primary goal of this lab is to learn how to leverage Azure Machine Learning (AML) to provision compute resources to train machine learning models, and then deploy the trained models either to a managed Azure Container Instance (ACI) or to a containerized platform such as Azure Kubernetes Services (AKS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ff47c",
   "metadata": {},
   "source": [
    "**Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import joblib\n",
    "import math\n",
    "import json\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.dataset_factory import TabularDatasetFactory\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice, AksWebservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "print('The azureml.core version is {}'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b9117",
   "metadata": {},
   "source": [
    "## Connect to the Azure Machine Learning Workspace\n",
    "\n",
    "The AML Python SDK is required for leveraging the experimentation, model management and model deployment capabilities of Azure Machine Learning services. Run the following cell to connect to the AML **Workspace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8585c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"The workspace name is: {}\".format(ws.name))\n",
    "print(\"The workspace resource group is: {}\".format(ws.resource_group))\n",
    "print(\"The workspace region is: {}\".format(ws.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420747f9",
   "metadata": {},
   "source": [
    "### Upload the training data to the blob store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d93d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = \"./data\"\n",
    "target_path = \"training-data\"\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(input_location, \n",
    "                 target_path = target_path, \n",
    "                 overwrite = True, \n",
    "                 show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0f781",
   "metadata": {},
   "source": [
    "### Create a Tabular dataset and review the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ef6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = DataPath(datastore=datastore, \n",
    "                              path_on_datastore=os.path.join(target_path, \"nyc-taxi-data.csv\"),\n",
    "                              name=\"training-data\")\n",
    "train_ds = Dataset.Tabular.from_delimited_files(path=training_data_path)\n",
    "train_ds.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9c88",
   "metadata": {},
   "source": [
    "### Register the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f361a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nyc-taxi-dataset\"\n",
    "description = \"Dataset to predict NYC taxi fares.\"\n",
    "registered_dataset = train_ds.register(ws, dataset_name, description=description, create_new_version=True)\n",
    "print('Registered dataset name {} and version {}'.format(registered_dataset.name, registered_dataset.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e25a61",
   "metadata": {},
   "source": [
    "## Create New Compute Cluster\n",
    "\n",
    "AML Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. In Azure Machine Learning there are two options to create a compute cluster to run your model training jobs. First option is to use the AML Studio to create the compute cluster and the second option is to use the AML Python SDK to create the compute cluster. Let’s review both approaches below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8d6b2",
   "metadata": {},
   "source": [
    "### Option #1: Create compute cluster from AML Studio\n",
    "\n",
    "- From within the AML Studio, navigate to **Compute, Compute clusters** and then select **+ New**\n",
    "\n",
    "![Create new compute cluster](./images/create_amlcompute_01.png 'Create New Compute Cluster')\n",
    "\n",
    "- In the **Select virtual machine** dialog, make the following selections and then select **Next**:\n",
    "    - Location: `Select a location closest to your AML workspace location`\n",
    "    - Virtual machine size: **Standard_DS12_v2**\n",
    "    \n",
    "![Create new compute cluster - Select virtual machine](./images/create_amlcompute_02.png 'Select Virtual Machine')\n",
    "\n",
    "- In the **Configure Settings** dialog, make the following selections and then select **Create**:\n",
    "    - Compute name: **amlcompute-ad**\n",
    "    - Minimum number of nodes: **0**\n",
    "    - Maximum number of nodes: **2**\n",
    "    \n",
    "![Create new compute cluster - Configure Settings](./images/create_amlcompute_03.png 'Configure Settings')\n",
    "    \n",
    "It will take few minutes to provision the AML Compute Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2cbcce",
   "metadata": {},
   "source": [
    "### Option #2: Create compute cluster using AML Python SDK\n",
    "\n",
    "Run the following cell to create a new AML compute cluster named: `amlcompute-ad`. Note that if you already created a new compute cluster from the AML studio, the below code will simply access that existing cluster, if not, it will create a new compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2', min_nodes=0, max_nodes=2)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed50a3",
   "metadata": {},
   "source": [
    "## Remotely train the machine learning model using the AML Compute Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeabd5a",
   "metadata": {},
   "source": [
    "### Create the training script\n",
    "\n",
    "The training script builds and trains the machine learning model. Review the code below to understand how we are using the `GradientBoostingRegressor` algorithm from the scikit-learn library to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information. After training the model, the script will register the trained model in the AML model registry with the name: ` nyc-taxi-fare-predictor`.\n",
    "\n",
    "Run the next two cells to create and save the training script `train.py` in the `scripts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_folder = './scripts'\n",
    "script_file_name = 'train.py'\n",
    "script_file_full_path = os.path.join(script_file_folder, script_file_name)\n",
    "os.makedirs(script_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc85077",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file_full_path\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core import Dataset\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"In train.py\")\n",
    "print(\"As a data scientist, this is where I write my training code.\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "\n",
    "parser.add_argument(\"--dataset_name\", type=str, help=\"dataset name\", dest=\"dataset_name\", required=True)\n",
    "parser.add_argument(\"--model_name\", type=str, help=\"model name\", dest=\"model_name\", required=True)\n",
    "parser.add_argument(\"--model_description\", type=str, help=\"model desc\", dest=\"model_description\", required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1: %s\" % args.dataset_name)\n",
    "print(\"Argument 2: %s\" % args.model_name)\n",
    "print(\"Argument 3: %s\" % args.model_description)\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "input_dataset = ws.datasets[args.dataset_name]\n",
    "print('Dataset name {} and version {}'.format(args.dataset_name, input_dataset.version))\n",
    "data = input_dataset.to_pandas_dataframe()\n",
    "print('Training data loaded!')\n",
    "\n",
    "x_df = data.drop(['totalAmount'], axis=1)\n",
    "y_df = data['totalAmount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=0)\n",
    "\n",
    "numerical = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', \n",
    "             'day_of_week', 'day_of_month', 'month_num', \n",
    "             'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n",
    "\n",
    "categorical = ['normalizeHolidayName', 'isPaidTimeOff']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])) for f in numerical]\n",
    "\n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = numeric_transformations + categorical_transformations\n",
    "\n",
    "# df_out will return a data frame, and default = None will pass the engineered features unchanged\n",
    "mapper = DataFrameMapper(transformations, input_df=True, df_out=True, default=None, sparse=False)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', mapper),\n",
    "                      ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_actual = y_test.values.flatten().tolist()\n",
    "rmse = math.sqrt(mean_squared_error(y_actual, y_predict))\n",
    "run.log('RMSE', rmse, 'Model RMSE on test set')\n",
    "print('The RMSE score on test data for GradientBoostingRegressor: ', rmse)\n",
    "\n",
    "output_folder = './outputs'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_filename = os.path.join(output_folder, 'nyc-taxi-fare.pkl')\n",
    "pickle.dump(clf, open(output_filename, 'wb'))\n",
    "print('Model file nyc-taxi-fare.pkl saved!')\n",
    "\n",
    "modelfiles_folder = output_folder\n",
    "model_name = args.model_name\n",
    "model_description = args.model_description #'Model to predict taxi fares in NYC.'\n",
    "\n",
    "os.chdir(modelfiles_folder)\n",
    "datasheet = {\"Type\": \"GradientBoostingRegressor\", \n",
    "             \"Run id\": run.id, \n",
    "             \"Training dataset name\": input_dataset.name, \n",
    "             \"Training dataset version\": input_dataset.version, \n",
    "             \"RMSE score\": rmse}\n",
    "\n",
    "model = Model.register(\n",
    "    model_path='nyc-taxi-fare.pkl',  # this points to a local file\n",
    "    model_name=model_name,  # this is the name the model is registered as\n",
    "    tags=datasheet,\n",
    "    description=model_description, \n",
    "    datasets=[('training data', input_dataset)], \n",
    "    workspace=ws\n",
    ")\n",
    "\n",
    "print(\"Model registered: {} \\nModel Description: {} \\nModel Version: {}\".format(model.name, \n",
    "                                                                                model.description, \n",
    "                                                                                model.version))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c1e2c",
   "metadata": {},
   "source": [
    "### Create and register the Model Training Environment\n",
    "\n",
    "AML environments are an encapsulation of the environment where your machine learning training happens. They define Python packages, environment variables, Docker settings and other attributes in declarative fashion. Environments are versioned: you can update them and retrieve old versions to revisit and review your work.\n",
    "\n",
    "Environments allow you to:\n",
    "* Encapsulate dependencies of your training process, such as Python packages and their versions.\n",
    "* Reproduce the Python environment on your local computer in a remote run on VM or ML Compute cluster\n",
    "* Reproduce your experimentation environment in production setting.\n",
    "* Revisit and audit the environment in which an existing model was trained.\n",
    "\n",
    "Environment, compute target and training script together form run configuration: the full specification of training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = Environment.get(workspace=ws, name='AzureML-Minimal').clone('Custom-Train-Env')\n",
    "cd = train_env.python.conda_dependencies\n",
    "cd.add_pip_package(\"numpy\")\n",
    "cd.add_pip_package(\"pandas\")\n",
    "cd.add_pip_package(\"joblib\")\n",
    "cd.add_pip_package(\"scikit-learn==0.24.1\")\n",
    "cd.add_pip_package(\"sklearn-pandas==2.2.0\")\n",
    "train_env.register(workspace=ws)\n",
    "print('Registered training env.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302919ff",
   "metadata": {},
   "source": [
    "### Create the ScriptRunConfig with the custom Enviroment\n",
    "\n",
    "In this case we pass the following parameters to the training script:\n",
    "\n",
    "- **dataset_name**: Name of the registered dataset to use for model training\n",
    "- **model_name**: Name of the model to use in the AML model registry\n",
    "- **model_description**: Model description to save with the registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa25fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'nyc-taxi-dataset'\n",
    "model_name = 'nyc-taxi-fare-predictor'\n",
    "model_description = 'Model to predict taxi fares in NYC.'\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_file_folder, \n",
    "                      script=script_file_name, \n",
    "                      arguments=['--dataset_name', dataset_name,\n",
    "                                 '--model_name', model_name,\n",
    "                                 '--model_description', model_description\n",
    "                                ], \n",
    "                      compute_target=compute_target, \n",
    "                      environment=train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967160da",
   "metadata": {},
   "source": [
    "### Submit the training run\n",
    "\n",
    "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'lab01-exp'\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "run = experiment.submit(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b51ef",
   "metadata": {},
   "source": [
    "### Monitor the Run Metrics\n",
    "\n",
    "Using the azureml Jupyter widget, you can monitor the training run. Run the cell below to monitor the experiment run. Wait till the model training is completed and the experiment run status is **Completed** before proceeding beyond the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f52f3",
   "metadata": {},
   "source": [
    "## Deploy the model to Azure Container Instance as a Web Service\n",
    "\n",
    "You can deploy a model as a real-time web service to several kinds of compute target, including local compute, an Azure Machine Learning compute instance, an Azure Container Instance (ACI), an Azure Kubernetes Service (AKS) cluster, an Azure Function, or an Internet of Things (IoT) module. In the section we will review how to deploy the model to **ACI** that is typically used for low-scale CPU-based workloads that require less than 48 GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cc2cb",
   "metadata": {},
   "source": [
    "### Create the scoring web service\n",
    "\n",
    "When deploying models for scoring with Azure Machine Learning services, you need to define the code for a simple web service that will load your model and use it for scoring. By convention this service has two methods init which loads the model and run which scores data using the loaded model.\n",
    "\n",
    "This scoring service code will later be deployed inside of a specially prepared Docker container. Run the cell below to create the scoring script `score.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n",
    "            'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', \n",
    "            'temperature', 'normalizeHolidayName', 'isPaidTimeOff']\n",
    "\n",
    "def init():\n",
    "\n",
    "    global trained_model\n",
    "    model_path = Model.get_model_path('nyc-taxi-fare-predictor')\n",
    "    trained_model = joblib.load(model_path)\n",
    "    print('model loaded')\n",
    "\n",
    "def run(input_json):\n",
    "    # Get predictions and explanations for each data point\n",
    "    inputs = json.loads(input_json)\n",
    "    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n",
    "    # Make prediction\n",
    "    predictions = trained_model.predict(data_df)\n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    return {'predictions': predictions.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33024a9a",
   "metadata": {},
   "source": [
    "### Create and register the Inferencing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_env = Environment(name='Custom-Inference-Env')\n",
    "cd = CondaDependencies()\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.add_pip_package(\"inference-schema\")\n",
    "cd.add_pip_package(\"numpy\")\n",
    "cd.add_pip_package(\"pandas\")\n",
    "cd.add_pip_package(\"joblib\")\n",
    "cd.add_pip_package(\"scikit-learn==0.24.1\")\n",
    "cd.add_pip_package(\"sklearn-pandas==2.2.0\")\n",
    "inference_env.python.conda_dependencies = cd\n",
    "inference_env.register(workspace=ws)\n",
    "print('Registered inferencing env.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c7f78",
   "metadata": {},
   "source": [
    "### Load the Registered Model\n",
    "\n",
    "Load the model that was registered during model training from the AML model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nyc-taxi-fare-predictor'\n",
    "registered_model = Model(ws, name=model_name)\n",
    "registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dbc48",
   "metadata": {},
   "source": [
    "### Package Model and deploy to ACI\n",
    "\n",
    "The steps involved include:\n",
    "- Create the inference config that specifies the scoring script and the deployment environment\n",
    "- Create the deployment configuration that specifies the characteristics of the compute\n",
    "-  Finally, deploy the model that specifies the registered model to deploy, the inference config and the deployment config\n",
    "\n",
    "Run the following two cell:  you may be waiting 5-15 minutes for completion, while the _Running_ tag adds progress dots.\n",
    "\n",
    "You will see output similar to the following when your web service is ready: \n",
    "\n",
    "`\n",
    "Succeeded\n",
    "ACI service creation operation finished, operation \"Succeeded\"\n",
    "Healthy\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9eb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(entry_script='score.py', source_directory='./', environment=inference_env)\n",
    "\n",
    "description = 'NYC Taxi Fare Predictor ACI Service'\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "                        cpu_cores=3, \n",
    "                        memory_gb=15, \n",
    "                        location='eastus', \n",
    "                        description=description, \n",
    "                        auth_enabled=False, \n",
    "                        tags = {'name': 'ACI container', \n",
    "                                'model_name': registered_model.name, \n",
    "                                'model_version': registered_model.version\n",
    "                                }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14521711",
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service_name='nyc-taxi-aci-service'\n",
    "\n",
    "aci_service = Model.deploy(workspace=ws,\n",
    "                           name=aci_service_name,\n",
    "                           models=[registered_model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config= aci_config, \n",
    "                           overwrite=True)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output=True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92f179",
   "metadata": {},
   "source": [
    "### Test Deployment\n",
    "\n",
    "Test your deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [1, 2, 5, 9, 4, 27, 5, 0, 0.0, 0.0, 65, 'Memorial Day', True]\n",
    "\n",
    "data2 = [[1, 3, 10, 15, 4, 27, 7, 0, 2.0, 1.0, 80, 'None', False], \n",
    "         [1, 2, 5, 9, 4, 27, 5, 0, 0.0, 0.0, 65, 'Memorial Day', True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a740bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aci_service.run(json.dumps(data1))\n",
    "print('Predictions for data1')\n",
    "print(result)\n",
    "result = aci_service.run(json.dumps(data2))\n",
    "print('Predictions for data2')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe447c",
   "metadata": {},
   "source": [
    "## Deploy the model to Azure Kubernetes Service as a Web Service\n",
    "\n",
    "**Azure Kubernetes Service (AKS)** is used for high-scale production deployments. In this section we will review how provision an AKS cluster and then how to deploy the registered model as a scoring web service to the AKS cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a727450",
   "metadata": {},
   "source": [
    "### Create AKS Cluster for Production Deployment\n",
    "\n",
    "In Azure Machine Learning there are two options to create an AKS cluster to deploy your trained models. First option is to use the AML Studio and the second option is to use the AML Python SDK. Let’s review both approaches below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac6484",
   "metadata": {},
   "source": [
    "### Option #1: Create an AKS cluster from AML Studio\n",
    "\n",
    "- From within the Azure Machine Learning Studio, navigate to **Compute, Inference Clusters** and select **+ New**\n",
    "\n",
    "![Create new inference cluster](./images/setup-aks-01.png 'Create New Inference Cluster')\n",
    "\n",
    "- In the **Select virtual machine** dialog, make the following selections and then select **Next**:\n",
    "    - Location: `Select a location closest to your AML workspace location`\n",
    "    - Virtual machine size: **Standard_D3_v2**\n",
    "    \n",
    "![Create new inference cluster - Select virtual machine](./images/setup-aks-02.png 'Select Virtual Machine')\n",
    "\n",
    "- In the **Configure Settings** dialog, make the following selections and then select **Create**:\n",
    "    - Compute name: **aks-cluster01**\n",
    "    - Cluster purpose: **Dev-test**\n",
    "    - Number of nodes: **1**\n",
    "    - Network configuration: **Basic**\n",
    "    \n",
    "![Create new inference cluster - Configure Settings](./images/setup-aks-03.png 'Configure Settings')\n",
    "\n",
    "**Note**: It can take several minutes to provision the inference cluster. Please wait for the cluster to be ready before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53bfa0",
   "metadata": {},
   "source": [
    "### Option #2: Create an AKS cluster using AML Python SDK\n",
    "\n",
    "Run the following cell to create an AKS cluster: `aks-cluster01`. Note that if you already created the AKS cluster from the AML studio, the below code will simply access that existing cluster, if not, it will create a new AKS cluster.\n",
    "\n",
    "In the cell below please specify the **aks_region** that is closest to your AML Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85714d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_name = \"aks-cluster01\"\n",
    "aks_region = \"eastus2\"\n",
    "compute_list = ws.compute_targets\n",
    "aks_target = None\n",
    "if aks_name in compute_list:\n",
    "    aks_target = compute_list[aks_name]\n",
    "else:\n",
    "    print(\"No AKS found. Creating new AKS: {} for model deployment.\".format(aks_name))\n",
    "    prov_config = AksCompute.provisioning_configuration(location=aks_region)\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n",
    "    aks_target.wait_for_completion(show_output=True)\n",
    "    print(aks_target.provisioning_state)\n",
    "    print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8740641",
   "metadata": {},
   "source": [
    "### Package Model and deploy to AKS\n",
    "\n",
    "Run the following two cell:  you may be waiting 5-15 minutes for completion, while the _Running_ tag adds progress dots.\n",
    "\n",
    "You will see output similar to the following when your web service is ready: \n",
    "\n",
    "`\n",
    "Succeeded\n",
    "AKS service creation operation finished, operation \"Succeeded\"\n",
    "Healthy\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e53f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'NYC Taxi Fare Predictor AKS Service'\n",
    "aks_config = AksWebservice.deploy_configuration(description = description, \n",
    "                                                tags = {'name': 'AKS container', \n",
    "                                                        'model_name': registered_model.name, \n",
    "                                                        'model_version': registered_model.version\n",
    "                                                       }\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service_name='nyc-taxi-aks-service'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[registered_model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config= aks_config, \n",
    "                           deployment_target=aks_target, \n",
    "                           overwrite=True)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output=True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa37634",
   "metadata": {},
   "source": [
    "### Test Deployment\n",
    "\n",
    "Finally, test your deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72352d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [1, 2, 5, 9, 4, 27, 5, 0, 0.0, 0.0, 65, 'Memorial Day', True]\n",
    "\n",
    "data2 = [[1, 3, 10, 15, 4, 27, 7, 0, 2.0, 1.0, 80, 'None', False], \n",
    "         [1, 2, 5, 9, 4, 27, 5, 0, 0.0, 0.0, 65, 'Memorial Day', True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287898a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aks_service.run(json.dumps(data1))\n",
    "print('Predictions for data1')\n",
    "print(result)\n",
    "result = aks_service.run(json.dumps(data2))\n",
    "print('Predictions for data2')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5f7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
